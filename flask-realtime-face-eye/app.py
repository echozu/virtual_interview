# -*- coding: utf-8 -*-
import base64
import random
import string
import threading

# å¯¼å…¥å¿…è¦çš„åº“
from flask import Flask, request, jsonify
import cv2
import numpy as np
import dlib
from math import hypot
import os
import time
import uuid  # ç”¨äºç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
from flask_cors import CORS  # å¯¼å…¥CORS
from celery import Celery

# å¯¼å…¥æˆ‘ä»¬æ–°çš„åˆ†ææ¨¡å—
from analysis_module import analyze_interview_data
import requests
# --- åˆå§‹åŒ– ---

# 1. åˆå§‹åŒ– Flask åº”ç”¨
app = Flask(__name__)
CORS(app)

# 2. é…ç½®åº”ç”¨å‚æ•°
app.config['MAIN_BACKEND_URL'] = 'http://123.207.53.16:9527/api/interview/process/python/video_analyse'
UPLOAD_FOLDER = 'uploads'
if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

# 3. é…ç½® Celery
app.config.update(
    broker_url='redis://:***REMOVED***@123.207.53.16:6379/9',
    result_backend='redis://:***REMOVED***@123.207.53.16:6379/9',
    result_expires=600  # å•ä½ï¼šç§’ï¼ˆ10åˆ†é’Ÿ = 600ç§’ï¼‰
)

# åˆ›å»º Celery å®ä¾‹
celery = Celery(app.name)
# ä» Flask çš„ app.config ä¸­åŠ è½½é…ç½®
celery.conf.update(app.config)


# --- 4. åŠ è½½ dlib æ¨¡å‹å’Œå®šä¹‰å¸¸é‡ ---
try:
    # âœ… [ä¿®æ­£] æ”¹ä¸ºä½¿ç”¨ç»å¯¹è·¯å¾„åŠ è½½æ¨¡å‹æ–‡ä»¶ï¼Œç¡®ä¿ Celery worker èƒ½åœ¨ä»»ä½•ä½ç½®æ‰¾åˆ°å®ƒ
    # è·å– app.py æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•çš„ç»å¯¹è·¯å¾„
    basedir = os.path.abspath(os.path.dirname(__file__))
    # æ„å»ºæ¨¡å‹æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    shape_predictor_path = os.path.join(basedir, "shape_predictor_68_face_landmarks.dat")

    if not os.path.exists(shape_predictor_path):
         print(f"é”™è¯¯ï¼šæ— æ³•åœ¨ä»¥ä¸‹è·¯å¾„æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {shape_predictor_path}")
         print("è¯·ç¡®ä¿ 'shape_predictor_68_face_landmarks.dat' æ–‡ä»¶ä¸ app.py å­˜åœ¨äºåŒä¸€ä¸ªç›®å½•ã€‚")
         exit()

    detector = dlib.get_frontal_face_detector()
    predictor = dlib.shape_predictor(shape_predictor_path)

except RuntimeError as e:
    print(f"åŠ è½½ dlib æ¨¡å‹æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
    print("è¿™é€šå¸¸æ˜¯ç”±äºæ¨¡å‹æ–‡ä»¶æŸåæˆ–ä¸dlibç‰ˆæœ¬ä¸å…¼å®¹å¯¼è‡´çš„ã€‚")
    exit()


BLINK_RATIO_THRESHOLD = 4.5
FRAME_PROCESSING_INTERVAL = 3
FRAME_RESIZE_FACTOR = 0.5


# --- è¾…åŠ©å‡½æ•° ---

def midpoint(p1, p2):
    return int((p1.x + p2.x) / 2), int((p1.y + p2.y) / 2)


def get_blinking_ratio(eye_points, facial_landmark):
    left_point = (facial_landmark.part(eye_points[0]).x, facial_landmark.part(eye_points[0]).y)
    right_point = (facial_landmark.part(eye_points[3]).x, facial_landmark.part(eye_points[3]).y)
    center_top = midpoint(facial_landmark.part(eye_points[1]), facial_landmark.part(eye_points[2]))
    center_bottom = midpoint(facial_landmark.part(eye_points[5]), facial_landmark.part(eye_points[4]))
    hor_line_length = hypot((left_point[0] - right_point[0]), (left_point[1] - right_point[1]))
    vert_line_length = hypot((center_top[0] - center_bottom[0]), (center_top[1] - center_bottom[1]))
    return hor_line_length / vert_line_length if vert_line_length != 0 else 100.0


def get_gaze_ratio(frame, eye_points, facial_landmark, gray):
    left_eye_region = np.array([(facial_landmark.part(p).x, facial_landmark.part(p).y) for p in eye_points], np.int32)
    height, width, _ = frame.shape
    mask = np.zeros((height, width), np.uint8)
    cv2.polylines(mask, [left_eye_region], True, 255, 2)
    cv2.fillPoly(mask, [left_eye_region], 255)
    eye = cv2.bitwise_and(gray, gray, mask=mask)
    min_x, max_x = np.min(left_eye_region[:, 0]), np.max(left_eye_region[:, 0])
    min_y, max_y = np.min(left_eye_region[:, 1]), np.max(left_eye_region[:, 1])
    gray_eye = eye[min_y:max_y, min_x:max_x]
    if gray_eye.size == 0: return None
    _, threshold_eye = cv2.threshold(gray_eye, 70, 255, cv2.THRESH_BINARY)
    threshold_eye = cv2.bitwise_not(threshold_eye)
    height, width = threshold_eye.shape
    if width == 0: return None
    left_side_threshold = threshold_eye[0:height, 0:int(width / 2)]
    left_side_white = cv2.countNonZero(left_side_threshold)
    right_side_threshold = threshold_eye[0:height, int(width / 2):width]
    right_side_white = cv2.countNonZero(right_side_threshold)
    return left_side_white / right_side_white if right_side_white != 0 else (1 if left_side_white > 0 else None)


def get_head_pose(shape, frame_shape):
    model_points = np.array([(0.0, 0.0, 0.0), (0.0, -330.0, -65.0), (-225.0, 170.0, -135.0), (225.0, 170.0, -135.0),
                             (-150.0, -150.0, -125.0), (150.0, -150.0, -125.0)])
    image_points = np.array(
        [(shape.part(30).x, shape.part(30).y), (shape.part(8).x, shape.part(8).y), (shape.part(36).x, shape.part(36).y),
         (shape.part(45).x, shape.part(45).y), (shape.part(48).x, shape.part(48).y),
         (shape.part(54).x, shape.part(54).y)], dtype="double")
    focal_length, center = frame_shape[1], (frame_shape[1] / 2, frame_shape[0] / 2)
    camera_matrix = np.array([[focal_length, 0, center[0]], [0, focal_length, center[1]], [0, 0, 1]], dtype="double")
    (success, rotation_vector, translation_vector) = cv2.solvePnP(model_points, image_points, camera_matrix,
                                                                  np.zeros((4, 1)), flags=cv2.SOLVEPNP_ITERATIVE)
    rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
    sy = np.sqrt(rotation_matrix[0, 0] * rotation_matrix[0, 0] + rotation_matrix[1, 0] * rotation_matrix[1, 0])
    singular = sy < 1e-6
    x = np.arctan2(rotation_matrix[2, 1], rotation_matrix[2, 2])
    y = np.arctan2(-rotation_matrix[2, 0], sy)
    z = np.arctan2(rotation_matrix[1, 0], rotation_matrix[0, 0]) if not singular else 0
    return np.degrees(x), np.degrees(y), np.degrees(z)


def get_facial_expression_proxies(landmarks):
    lip_top, lip_bottom = (landmarks.part(51).x, landmarks.part(51).y), (landmarks.part(57).x, landmarks.part(57).y)
    mouth_opening = hypot(lip_top[0] - lip_bottom[0], lip_top[1] - lip_bottom[1])
    mouth_left, mouth_right = (landmarks.part(48).x, landmarks.part(48).y), (landmarks.part(54).x, landmarks.part(54).y)
    mouth_width = hypot(mouth_left[0] - mouth_right[0], mouth_left[1] - mouth_right[1])
    face_width = hypot(landmarks.part(0).x - landmarks.part(16).x, landmarks.part(0).y - landmarks.part(16).y)
    if face_width == 0: return {'mouth_opening_ratio': 0, 'mouth_smile_ratio': 0}
    return {'mouth_opening_ratio': mouth_opening / face_width, 'mouth_smile_ratio': mouth_width / face_width}


# --- ä¸»è§†é¢‘å¤„ç†å‡½æ•° ---
def process_video(video_path):
    if not os.path.exists(video_path):
        print(f"é”™è¯¯: è§†é¢‘æ–‡ä»¶åœ¨è·¯å¾„ {video_path} ä¸å­˜åœ¨ï¼")
        return None

    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print(f"é”™è¯¯: OpenCV (cv2.VideoCapture) æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}ã€‚")
        print("-> å¯èƒ½çš„åŸå› æ˜¯ï¼šç¯å¢ƒç¼ºå°‘å¤„ç†.webmæ ¼å¼æ‰€éœ€çš„è§†é¢‘è§£ç å™¨(å¦‚VP9)ã€‚è¯·æ£€æŸ¥FFmpegæ˜¯å¦æ­£ç¡®å®‰è£…ã€‚")
        return None

    print(f"è§†é¢‘æ–‡ä»¶ {video_path} å·²æˆåŠŸæ‰“å¼€ï¼Œå¼€å§‹é€å¸§å¤„ç†ã€‚")

    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps == 0:
        print("è­¦å‘Š: æ— æ³•è·å–è§†é¢‘çš„FPSï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ 30ã€‚")
        fps = 30

    analysis_results_by_second = []
    frame_data_in_second = []
    current_second = 0
    total_frame_count = 0
    is_blinking = False

    first_frame_base64 = None
    last_frame_np = None

    while True:
        ret, frame = cap.read()
        if not ret:
            print("è§†é¢‘å¤„ç†å®Œæˆï¼šå·²è¯»å–åˆ°æ‰€æœ‰å¸§ã€‚")
            break

        total_frame_count += 1

        if total_frame_count % FRAME_PROCESSING_INTERVAL != 0:
            continue

        if first_frame_base64 is None:
            success, buffer = cv2.imencode('.jpg', frame)
            if success:
                first_frame_base64 = base64.b64encode(buffer).decode('utf-8')

        last_frame_np = frame.copy()

        second_marker = int((total_frame_count - 1) / fps)

        if second_marker > current_second:
            if frame_data_in_second:
                attention_scores = [d['attention_score'] for d in frame_data_in_second]
                valid_frames = [d for d in frame_data_in_second if d['face_detected']]
                pitches = [d['head_pose']['pitch'] for d in valid_frames if 'pitch' in d['head_pose']]
                yaws = [d['head_pose']['yaw'] for d in valid_frames if 'yaw' in d['head_pose']]
                rolls = [d['head_pose']['roll'] for d in valid_frames if 'roll' in d['head_pose']]
                mouth_openings = [d['expression']['mouth_opening_ratio'] for d in valid_frames]
                mouth_smiles = [d['expression']['mouth_smile_ratio'] for d in valid_frames]

                analysis_results_by_second.append({
                    "time_in_seconds": current_second + 1,
                    "face_detection_rate": len(valid_frames) / len(frame_data_in_second) if frame_data_in_second else 0,
                    "attention_mean": np.mean(attention_scores) if attention_scores else 0,
                    "attention_std": np.std(attention_scores) if attention_scores else 0,
                    "blink_count": sum(d['is_blinking'] for d in frame_data_in_second),
                    "head_pose_mean": {"pitch": np.mean(pitches) if pitches else 0, "yaw": np.mean(yaws) if yaws else 0,
                                       "roll": np.mean(rolls) if rolls else 0},
                    "head_pose_std": {"pitch": np.std(pitches) if pitches else 0, "yaw": np.std(yaws) if yaws else 0,
                                      "roll": np.std(rolls) if rolls else 0},
                    "expression_mean": {"mouth_opening_ratio": np.mean(mouth_openings) if mouth_openings else 0,
                                        "mouth_smile_ratio": np.mean(mouth_smiles) if mouth_smiles else 0}
                })
            frame_data_in_second = []
            current_second = second_marker

        small_frame = cv2.resize(frame, (0, 0), fx=FRAME_RESIZE_FACTOR, fy=FRAME_RESIZE_FACTOR)
        gray = cv2.cvtColor(small_frame, cv2.COLOR_BGR2GRAY)

        faces = detector(gray)

        frame_analysis = {"face_detected": False, "attention_score": 0.0, "is_blinking": False, "head_pose": {},
                          "expression": {}}
        if len(faces) > 0:
            landmarks_small = predictor(gray, faces[0])
            points = [dlib.point(int(p.x / FRAME_RESIZE_FACTOR), int(p.y / FRAME_RESIZE_FACTOR)) for p in
                      landmarks_small.parts()]
            landmarks = dlib.full_object_detection(faces[0], points)

            frame_analysis.update({
                "face_detected": True,
                "attention_score": 0.3,
                "head_pose": dict(zip(['pitch', 'yaw', 'roll'], get_head_pose(landmarks, frame.shape))),
                "expression": get_facial_expression_proxies(landmarks)
            })

            blinking_ratio = (get_blinking_ratio([36, 37, 38, 39, 40, 41], landmarks) + get_blinking_ratio(
                [42, 43, 44, 45, 46, 47], landmarks)) / 2
            if blinking_ratio > BLINK_RATIO_THRESHOLD:
                if not is_blinking: frame_analysis["is_blinking"] = True
                is_blinking = True
            else:
                is_blinking = False

            original_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gaze_ratio = get_gaze_ratio(frame, [36, 37, 38, 39, 40, 41], landmarks, original_gray)
            if gaze_ratio is not None and 0.8 < gaze_ratio < 2.2:
                frame_analysis["attention_score"] = 1.0
            else:
                frame_analysis["attention_score"] = 0.0

        frame_data_in_second.append(frame_analysis)

    last_frame_base64 = None
    if last_frame_np is not None:
        success, buffer = cv2.imencode('.jpg', last_frame_np)
        if success:
            last_frame_base64 = base64.b64encode(buffer).decode('utf-8')

    cap.release()

    return {
        "video_duration_seconds": round(total_frame_count / fps, 2) if fps > 0 else 0,
        "analysis_by_second": analysis_results_by_second,
        "first_frame_base64": first_frame_base64,
        "last_frame_base64": last_frame_base64
    }


# æ–°å¢ï¼šæŒ‰æŒ‡å®šæ—¶é—´é—´éš”èšåˆæ•°æ®çš„å‡½æ•°
def aggregate_data_by_interval(analysis_by_second, interval_seconds=30):
    if not analysis_by_second: return []
    aggregated_results = []
    num_seconds = len(analysis_by_second)
    for i in range(0, num_seconds, interval_seconds):
        chunk = analysis_by_second[i:i + interval_seconds]
        if not chunk: continue

        start_time, end_time = chunk[0]['time_in_seconds'], chunk[-1]['time_in_seconds']
        attention_means = [s['attention_mean'] for s in chunk]
        pitch_means, yaw_means, roll_means = ([s['head_pose_mean']['pitch'] for s in chunk],
                                              [s['head_pose_mean']['yaw'] for s in chunk],
                                              [s['head_pose_mean']['roll'] for s in chunk])

        aggregated_results.append({
            "start_time_seconds": start_time,
            "end_time_seconds": end_time,
            "interval_duration_seconds": len(chunk),
            "face_detection_rate": round(np.mean([s['face_detection_rate'] for s in chunk]), 3),
            "attention_mean": round(np.mean(attention_means), 3),
            "attention_stability": round(np.std(attention_means), 3),
            "total_blinks": sum(s['blink_count'] for s in chunk),
            "head_pose": {
                "pitch_mean": round(np.mean(pitch_means), 2), "yaw_mean": round(np.mean(yaw_means), 2),
                "roll_mean": round(np.mean(roll_means), 2),
                "pitch_fluctuation": round(np.std(pitch_means), 2), "yaw_fluctuation": round(np.std(yaw_means), 2),
                "roll_fluctuation": round(np.std(roll_means), 2)
            },
            "expression": {
                "mouth_opening_mean": round(np.mean([s['expression_mean']['mouth_opening_ratio'] for s in chunk]), 4),
                "mouth_smile_mean": round(np.mean([s['expression_mean']['mouth_smile_ratio'] for s in chunk]), 4)
            }
        })
    return aggregated_results


def calculate_nervousness_score(half_minute_data):
    """
    æ ¹æ®åŠåˆ†é’Ÿçš„æ•°æ®å—ï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†çš„ã€ç»“æ„åŒ–çš„ç´§å¼ åº¦åˆ†ææŠ¥å‘Šã€‚
    è¿™ä»½æŠ¥å‘Šä¸ºäºŒæ¬¡AIåˆ†ææä¾›äº†é‡åŒ–çš„ç‰¹å¾ã€é˜ˆå€¼å’Œåˆ¤æ–­ä¾æ®ã€‚
    :param half_minute_data: analysis_by_half_minute åˆ—è¡¨ä¸­çš„ä¸€ä¸ªå…ƒç´ 
    :return: ä¸€ä¸ªåŒ…å«è¯¦ç»†åˆ†æçš„å­—å…¸
    """
    THRESHOLDS = {
        'blinks': {'normal_range': [8, 15], 'high_threshold': 20, 'unit': 'æ¬¡/åŠåˆ†é’Ÿ'},
        'attention_stability': {'high_threshold': 0.3, 'unit': 'æ ‡å‡†å·®'},
        'head_yaw_fluctuation': {'high_threshold': 4.0, 'unit': 'åº¦/æ ‡å‡†å·®'},
        'face_detection_rate': {'low_threshold': 0.9, 'unit': 'æ¯”ä¾‹'}
    }
    total_score = 0
    components = {}
    blinks_val = half_minute_data['total_blinks']
    blinks_config = THRESHOLDS['blinks']
    blinks_score = 0
    blinks_comment = "çœ¨çœ¼é¢‘ç‡åœ¨æ­£å¸¸èŒƒå›´å†…ã€‚"
    if blinks_val > blinks_config['high_threshold']:
        blinks_score = 3
        blinks_comment = "çœ¨çœ¼é¢‘ç‡æ˜¾è‘—é«˜äºæ­£å¸¸èŒƒå›´ï¼Œæ˜¯å‹åŠ›çš„å¼ºæŒ‡æ ‡ã€‚"
    elif blinks_val > blinks_config['normal_range'][1]:
        blinks_score = 1
        blinks_comment = "çœ¨çœ¼é¢‘ç‡åé«˜ï¼Œå¯èƒ½å­˜åœ¨è½»å¾®ç´§å¼ ã€‚"
    components['blinks'] = {
        "value": blinks_val, "unit": blinks_config['unit'],
        "normal_range": blinks_config['normal_range'],
        "score_contribution": blinks_score, "comment": blinks_comment
    }
    total_score += blinks_score
    attention_val = round(half_minute_data['attention_stability'], 3)
    attention_config = THRESHOLDS['attention_stability']
    attention_score = 0
    attention_comment = "æ³¨æ„åŠ›ç¨³å®šï¼Œè§†çº¿æ¥è§¦è‰¯å¥½ã€‚"
    if attention_val > attention_config['high_threshold']:
        attention_score = 2
        attention_comment = "æ³¨æ„åŠ›ç¨³å®šæ€§ä½äºé˜ˆå€¼ï¼Œè¡¨æ˜è§†çº¿å¯èƒ½å­˜åœ¨å›é¿æˆ–é£˜å¿½ã€‚"
    components['attention_stability'] = {
        "value": attention_val, "unit": attention_config['unit'],
        "threshold_high": attention_config['high_threshold'],
        "score_contribution": attention_score, "comment": attention_comment
    }
    total_score += attention_score
    yaw_fluctuation_val = round(half_minute_data['head_pose']['yaw_fluctuation'], 2)
    yaw_config = THRESHOLDS['head_yaw_fluctuation']
    yaw_score = 0
    yaw_comment = "å¤´éƒ¨å§¿æ€ç¨³å®šï¼Œæœªè§æ˜æ˜¾ä¸å®‰çš„å°åŠ¨ä½œã€‚"
    if yaw_fluctuation_val > yaw_config['high_threshold']:
        yaw_score = 2
        yaw_comment = "å¤´éƒ¨å·¦å³æ™ƒåŠ¨å¹…åº¦åé«˜ï¼Œå¯èƒ½æ˜¯ä¸å®‰æˆ–å°åŠ¨ä½œå¢å¤šçš„ä½“ç°ã€‚"
    components['head_yaw_fluctuation'] = {
        "value": yaw_fluctuation_val, "unit": yaw_config['unit'],
        "threshold_high": yaw_config['high_threshold'],
        "score_contribution": yaw_score, "comment": yaw_comment
    }
    total_score += yaw_score
    face_rate_val = round(half_minute_data['face_detection_rate'], 2)
    face_rate_config = THRESHOLDS['face_detection_rate']
    face_rate_score = 0
    face_rate_comment = "é¢éƒ¨æ£€å‡ºç‡æ­£å¸¸ï¼Œæœªå‘ç°æ˜æ˜¾çš„ç”¨æ‰‹é®æŒ¡è¡Œä¸ºã€‚"
    if face_rate_val < face_rate_config['low_threshold']:
        face_rate_score = 3
        face_rate_comment = "é¢éƒ¨æ£€å‡ºç‡è¾ƒä½ï¼Œå¯èƒ½å­˜åœ¨é¢‘ç¹ç”¨æ‰‹é®æŒ¡è„¸éƒ¨çš„è¡Œä¸ºï¼Œè¿™æ˜¯ç´§å¼ çš„å…¸å‹ä¿¡å·ã€‚"
    components['face_detection_rate'] = {
        "value": face_rate_val, "unit": face_rate_config['unit'],
        "threshold_low": face_rate_config['low_threshold'],
        "score_contribution": face_rate_score, "comment": face_rate_comment
    }
    total_score += face_rate_score
    if total_score >= 5:
        level = "é«˜åº¦ç´§å¼ "
    elif total_score >= 3:
        level = "ä¸­åº¦ç´§å¼ "
    elif total_score >= 1:
        level = "è½»å¾®ç´§å¼ "
    else:
        level = "çŠ¶æ€æ”¾æ¾"
    return {
        "overall_score": total_score,
        "level": level,
        "components": components
    }


# --- Celery ä»»åŠ¡å®šä¹‰ ---
@celery.task
def run_analysis_and_forward(video_path, session_id, analysis_id, main_backend_url):
    """
    è¿™ä¸ªå‡½æ•°ç°åœ¨æ˜¯ä¸€ä¸ª Celery ä»»åŠ¡ï¼Œå°†ç”± Celery worker åœ¨åå°æ‰§è¡Œã€‚
    """
    final_response_to_main_backend = None
    try:
        print(f"Celery ä»»åŠ¡ [{analysis_id}] å¼€å§‹åˆ†æè§†é¢‘: {video_path}")
        start_time = time.time()

        raw_results = process_video(video_path)

        if raw_results is None or not raw_results.get("analysis_by_second"):
            print(f"é”™è¯¯ [{analysis_id}]: è§†é¢‘å¤„ç†å¤±è´¥æˆ–æœªåœ¨è§†é¢‘ä¸­æ£€æµ‹åˆ°æœ‰æ•ˆæ´»åŠ¨ã€‚ä»»åŠ¡ç»ˆæ­¢ã€‚")
            error_response = {
                "analysisId": analysis_id,
                "sessionId": session_id,
                "status": "error",
                "message": "è§†é¢‘å¤„ç†å¤±è´¥ï¼šæ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶æˆ–è§†é¢‘å†…å®¹æ— æ³•åˆ†æã€‚"
            }
            try:
                requests.post(main_backend_url, json=error_response, timeout=20).raise_for_status()
                print(f"Celery ä»»åŠ¡ [{analysis_id}]: å·²æˆåŠŸå°†é”™è¯¯çŠ¶æ€è½¬å‘è‡³ä¸»åç«¯ã€‚")
            except requests.exceptions.RequestException as e:
                print(f"é”™è¯¯ [{analysis_id}]: è½¬å‘é”™è¯¯çŠ¶æ€è‡³ä¸»åç«¯å¤±è´¥: {e}")
            return

        analysis_data = raw_results["analysis_by_second"]
        half_minute_report = aggregate_data_by_interval(analysis_data, 30)
        for report_block in half_minute_report:
            nervousness_analysis = calculate_nervousness_score(report_block)
            report_block['nervousness_analysis'] = nervousness_analysis

        interview_summary = analyze_interview_data(half_minute_report)
        end_time = time.time()
        print(f"Celery ä»»åŠ¡ [{analysis_id}] åˆ†æå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")

        final_response_to_main_backend = {
            "analysisId": analysis_id,
            "sessionId": session_id,
            "status": "success",
            "message": "è§†é¢‘åˆ†ææˆåŠŸ",
            "analysis_by_half_minute": half_minute_report,
            "overall_summary": interview_summary,
            "first_frame_base64": raw_results.get("first_frame_base64"),
            "last_frame_base64": raw_results.get("last_frame_base64")
        }

        print(f"Celery ä»»åŠ¡ [{analysis_id}] æ­£åœ¨å°†ç²¾ç®€åçš„ç»“æœè½¬å‘è‡³: {main_backend_url}")
        try:
            requests.post(main_backend_url, json=final_response_to_main_backend, timeout=20).raise_for_status()
            print(f"Celery ä»»åŠ¡ [{analysis_id}] æˆåŠŸå°†æ•°æ®è½¬å‘è‡³ä¸»åç«¯ã€‚")
        except requests.exceptions.RequestException as e:
            print(f"é”™è¯¯ [{analysis_id}]: è½¬å‘è‡³ä¸»åç«¯å¤±è´¥: {e}")

    except Exception as e:
        print(f"Celery ä»»åŠ¡ [{analysis_id}] å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        # è¿™é‡Œä¹Ÿå¯ä»¥å‘ä¸»åç«¯å‘é€ä¸€ä¸ªé”™è¯¯æŠ¥å‘Š

    finally:
        if os.path.exists(video_path):
            try:
                os.remove(video_path)
                print(f"Celery ä»»åŠ¡ [{analysis_id}]: å·²åˆ é™¤ä¸´æ—¶è§†é¢‘æ–‡ä»¶ {video_path}")
            except OSError as e:
                print(f"é”™è¯¯ [{analysis_id}]: åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {video_path} å¤±è´¥: {e}")

        if final_response_to_main_backend:
            printable_response = final_response_to_main_backend.copy()
            printable_response.pop("first_frame_base64", None)
            printable_response.pop("last_frame_base64", None)
            print(f"Celery ä»»åŠ¡ [{analysis_id}]: å‘é€åç«¯çš„å€¼ä¸ºï¼š {printable_response}")

# --- Flask API è·¯ç”± ---
@app.route('/api/analyze_video', methods=['POST'])
def analyze_video_api():
    if 'video' not in request.files: return jsonify({"error": "è¯·æ±‚ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶"}), 400
    if 'sessionId' not in request.form: return jsonify({"error": "è¯·æ±‚ä¸­ç¼ºå°‘ sessionId"}), 400

    file = request.files['video']
    session_id = request.form.get('sessionId')
    if file.filename == '': return jsonify({"error": "æœªé€‰æ‹©æ–‡ä»¶"}), 400

    timestamp = int(time.time())
    random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))
    analysis_id = f"{session_id}-{timestamp}-{random_str}"
    # ç¡®ä¿æ–‡ä»¶åæ˜¯å®‰å…¨çš„ï¼Œè™½ç„¶æˆ‘ä»¬ç”¨äº†uuidï¼Œä½†è¿™æ˜¯ä¸ªå¥½ä¹ æƒ¯
    from werkzeug.utils import secure_filename
    safe_filename = secure_filename(f"{analysis_id}.webm")
    video_path = os.path.join(app.config['UPLOAD_FOLDER'], safe_filename)
    file.save(video_path)

    main_backend_url = app.config['MAIN_BACKEND_URL']
    run_analysis_and_forward.delay(video_path, session_id, analysis_id, main_backend_url)

    print(f"å·²ä¸ºä¼šè¯ {session_id} åˆ›å»º Celery ä»»åŠ¡ï¼ŒIDä¸º: {analysis_id}")

    return jsonify({
        "status": "processing",
        "message": "åˆ†æä»»åŠ¡å·²æˆåŠŸåˆ›å»ºï¼Œæ­£åœ¨åå°å¤„ç†ä¸­ã€‚",
        "analysisId": analysis_id
    })

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == '__main__':
    ip_address = "0.0.0.0"
    port = 55274
    print(f"âœ… APIæœåŠ¡å¯åŠ¨æˆåŠŸï¼Œæ­£åœ¨ç›‘å¬ http://{ip_address}:{port}")
    print(f"ğŸš€ è¯·é€šè¿‡ POST è¯·æ±‚è®¿é—® /api/analyze_video æ¥å£ä»¥ä¸Šä¼ è§†é¢‘è¿›è¡Œåˆ†æ")
    # åœ¨å¼€å‘ç¯å¢ƒä¸­ï¼ŒFlask çš„é‡è½½å™¨ (reloader) å¯èƒ½ä¼šå¯¼è‡´ dlib æ¨¡å‹è¢«åŠ è½½ä¸¤æ¬¡ã€‚
    # è®¾ç½® use_reloader=False å¯ä»¥é¿å…è¿™ä¸ªé—®é¢˜ï¼Œä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­é€šå¸¸ç”¨ Gunicorn ç­‰æœåŠ¡å™¨ï¼Œä¸å—æ­¤å½±å“ã€‚
    app.run(host=ip_address, port=port, debug=True, use_reloader=False)